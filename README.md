# Vision-speak
# VisionSpeak: Real-Time Video Translation with LFM2-VL

## Overview
VisionSpeak uses LiquidAI’s LFM2-VL model to interpret visual scenes and translate them live. It demonstrates “LFMs with Eyes” by combining vision and language in real-time.

## Features
- Live webcam feed
- Scene description via LFM2-VL
- Translation to French using NLLB
- Overlay of translated text on video

## How It Works
1. Captures video frame
2. Describes scene using LFM2-VL
3. Translates description
4. Displays translation on screen

## Tools Used
- LFM2-VL-1.6B (Vision-Language)
- NLLB-200 (Translation)
- OpenCV (Video)
- Python

## Demo
See screen recording in Drive folder.

## Submission
- GitHub: [link to your repo]
- Drive: [link to your folder]
